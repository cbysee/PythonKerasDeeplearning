{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter8-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ohIUN1Fi8GH-m2C0Dbtjq5awi0h0k5Jo",
      "authorship_tag": "ABX9TyNb5n7h2jezLD97RMOCQSj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CobaMasa/PythonKerasDeeplearning/blob/main/chapter8_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzRT2U_XnB2W"
      },
      "source": [
        "# Chapter 8-3: Style transfer with NN (Neural Style Transfer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyXUGH9-KNwG",
        "outputId": "01094a5f-3a30-4790-c753-ad69ea50d512"
      },
      "source": [
        "# change the version\n",
        "!pip install tensorflow==1.5.0\n",
        "!pip install keras==2.0.8"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.5.0 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (0.35.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-tensorboard<1.6.0,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (1.5.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.5.0) (50.3.2)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (0.9999999)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (1.0.1)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (1.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5.0) (3.4.0)\n",
            "Requirement already satisfied: keras==2.0.8 in /usr/local/lib/python3.6/dist-packages (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.0.8) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wgphaqkm-uk",
        "outputId": "5143518b-2b80-418e-bd87-58da3e633aa4"
      },
      "source": [
        "# List 8-14&15: Define variables\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications import vgg19\n",
        "\n",
        "# Set paths\n",
        "target_image_path = 'drive/MyDrive/Study/PythonKeras/images/cat.jpg'\n",
        "style_reference_image_path = 'drive/MyDrive/Study/PythonKeras/images/monet.jpg'\n",
        "\n",
        "# Set image size \n",
        "width, height, = load_img(target_image_path).size\n",
        "img_height = 800\n",
        "img_width = int(width*img_height/height)\n",
        "\n",
        "# def some compimentaly functions\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_height, img_width))\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD1BpX-8rVsp"
      },
      "source": [
        "# List 8-16: Get ready to apply the model to images\n",
        "import keras.backend as K\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))\n",
        "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
        "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
        "\n",
        "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n",
        "\n",
        "model  = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdKljzuvmw6"
      },
      "source": [
        "# List 8-17~20: define contents & style loss functions\n",
        "\n",
        "# loss for contents\n",
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination-base))\n",
        "\n",
        "# loss for style\n",
        "def gram_matrix(x):\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features, K.transpose(features))\n",
        "    return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_height * img_width\n",
        "    return K.sum(K.square(S-C)) / (4. * (channels**2) * (size**2))\n",
        "\n",
        "# whole loss\n",
        "def total_variation_loss(x):\n",
        "    a = K.square(x[:, :img_height-1, :img_width-1, :] - x[:, 1:, :img_width-1, :])\n",
        "    b = K.square(x[:, :img_height-1, :img_width-1, :] -x[:, :img_height-1, 1:, :])\n",
        "    return K.sum(K.pow(a+b, 1.25))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh2zXxlWzZb2",
        "outputId": "c5211ac8-8e3e-4e3e-e101-fc9841758b9b"
      },
      "source": [
        "# List 8-20\n",
        "outputs_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "content_layer = 'block5_conv2'\n",
        "style_layers = ['block{}_conv1'.format(i) for i in range(1, 6)]\n",
        "\n",
        "# add contents loss weight\n",
        "total_variation_weight = 1e-4\n",
        "style_weight = 1.\n",
        "content_weight = 0.025\n",
        "\n",
        "loss = K.variable(0.)\n",
        "\n",
        "layer_features = outputs_dict[content_layer]\n",
        "target_image_features = layer_features.output[0, :, :, :]\n",
        "combination_features = layer_features.output[2, :, :, :]\n",
        "loss = loss + content_weight * content_loss(target_image_features,\n",
        "                                      combination_features)\n",
        "for layer_name in style_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features.output[1, :, :, :]\n",
        "    combination_features = layer_features.output[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss += (style_weight / len(style_layers)) * sl\n",
        "loss = loss + total_variation_weight * total_variation_loss(combination_image)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTA-Mp4b4t8B"
      },
      "source": [
        "# List 8-21: Define Gradient discendent process\n",
        "grads = K.gradients(loss, combination_image)[0]\n",
        "\n",
        "# Function to fetch the values of the current loss and the current gradients\n",
        "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
        "\n",
        "\n",
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        x = x.reshape((1, img_height, img_width, 3))\n",
        "        outs = fetch_loss_and_grads([x])\n",
        "        loss_value = outs[0]\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values\n",
        "\n",
        "evaluator = Evaluator()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtwdur8v5Go7",
        "outputId": "7e6ed3e2-3077-4bf3-a5b8-88c92b959373"
      },
      "source": [
        "# List 8-22: Style transfer loop\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import imageio # substitute scipy.misc.imsave to this\n",
        "import time\n",
        "\n",
        "result_prefix = 'style_transfer_result'\n",
        "iterations = 20\n",
        "\n",
        "# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss.\n",
        "# This is our initial state: the target image.\n",
        "# Note that `scipy.optimize.fmin_l_bfgs_b` can only process flat vectors.\n",
        "x = preprocess_image(target_image_path)\n",
        "x = x.flatten()\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # Save current generated image\n",
        "    img = x.copy().reshape((img_height, img_width, 3))\n",
        "    img = deprocess_image(img)\n",
        "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "    imageio.imwrite(fname, img) # substitute scipy.misc.imsave to this\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of iteration 0\n",
            "Current loss value: 1929732600.0\n",
            "Image saved as style_transfer_result_at_iteration_0.png\n",
            "Iteration 0 completed in 876s\n",
            "Start of iteration 1\n",
            "Current loss value: 1205489900.0\n",
            "Image saved as style_transfer_result_at_iteration_1.png\n",
            "Iteration 1 completed in 849s\n",
            "Start of iteration 2\n",
            "Current loss value: 951356300.0\n",
            "Image saved as style_transfer_result_at_iteration_2.png\n",
            "Iteration 2 completed in 843s\n",
            "Start of iteration 3\n",
            "Current loss value: 777369900.0\n",
            "Image saved as style_transfer_result_at_iteration_3.png\n",
            "Iteration 3 completed in 843s\n",
            "Start of iteration 4\n",
            "Current loss value: 659198700.0\n",
            "Image saved as style_transfer_result_at_iteration_4.png\n",
            "Iteration 4 completed in 878s\n",
            "Start of iteration 5\n",
            "Current loss value: 571629500.0\n",
            "Image saved as style_transfer_result_at_iteration_5.png\n",
            "Iteration 5 completed in 857s\n",
            "Start of iteration 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fctwH5A6Z5jZ"
      },
      "source": [
        "## VAE: Valiational Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77iIFFB7HIos"
      },
      "source": [
        "# List 8-23: VAE networks\n",
        "import keras\n",
        "from kears.layers import Input, Conv2D, Flatten, Dense, Lambda\n",
        "from keras.models import Models\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "img_shape = (28, 28, 1)\n",
        "batch_size = 16\n",
        "latent_dim = 2\n",
        "\n",
        "input_img = Input(shape=(img_shape))\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(input_img))\n",
        "x = Conv2D(64, 3, padding='same', activation='relu', stride=(2, 2))(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu'(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu'(x)\n",
        "\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5DLJvfiLm4o"
      },
      "source": [
        "# List 8-24: samping function from latent space\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
        "    return z_mean + K.exp(z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling)([z_mean, z_log_var])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OrJdAT-cMPA"
      },
      "source": [
        "# List 8-25: VAE decoder network for mapping latent point to an image\n",
        "decoder_input = Input(K.int_shape(z)[1:])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}